#!/usr/bin/env python3
"""
ë ˆì´ë” ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ íƒì§€ ì‹œìŠ¤í…œ - SNR 15dB + k=6 + ê±°ë¦¬ 0.4m í•™ìŠµ
Modified RaGNNarok v2.0 - GPU ê°€ì† ì§€ì›

ì„¤ì •:
- SNR ì„ê³„ê°’: 15.0 dB (ë„ë©”ì¸ ì ì‘ ê· í˜• ê¸°ì¤€)
- ê±°ë¦¬ ì„ê³„ê°’: 0.4 m (ì¤‘ê°„ ì§€ì )
- k-NN ì—°ê²°: 6ê°œ (ì ì ˆí•œ ì´ì›ƒ ì˜í–¥)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import SAGEConv, BatchNorm
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
from dataclasses import dataclass
from typing import List, Tuple, Dict
import time
import os

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

@dataclass
class RadarPoint:
    """ë ˆì´ë” í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì¡°"""
    timestamp: float
    x: float
    y: float
    velocity: float
    snr: float

@dataclass
class LiDARPoint:
    """LiDAR í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì¡°"""
    timestamp: float
    x: float
    y: float
    intensity: float

# íƒ€ì… ì •ì˜
RadarFrame = List[RadarPoint]
LiDARFrame = List[LiDARPoint]

class HybridGhostGNN(nn.Module):
    """GraphSAGE ê¸°ë°˜ ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ íƒì§€ GNN ëª¨ë¸"""
    
    def __init__(self, input_dim=6, hidden_dim=128, num_layers=3, dropout=0.1):
        super(HybridGhostGNN, self).__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # GraphSAGE ë ˆì´ì–´ë“¤
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        self.convs.append(SAGEConv(input_dim, hidden_dim))
        self.batch_norms.append(BatchNorm(hidden_dim))
        
        # ì¤‘ê°„ ë ˆì´ì–´ë“¤
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))
            self.batch_norms.append(BatchNorm(hidden_dim))
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.convs.append(SAGEConv(hidden_dim, 1))
        
        self.dropout_layer = nn.Dropout(dropout)
        
    def forward(self, x, edge_index, batch=None):
        # ì¤‘ê°„ ë ˆì´ì–´ë“¤
        for i in range(self.num_layers - 1):
            x = self.convs[i](x, edge_index)
            x = self.batch_norms[i](x)
            x = torch.relu(x)
            x = self.dropout_layer(x)
        
        # ì¶œë ¥ ë ˆì´ì–´
        x = self.convs[-1](x, edge_index)
        x = torch.sigmoid(x)
        
        return x.squeeze()

def extract_features(radar_points: List[RadarPoint]) -> np.ndarray:
    """ë ˆì´ë” í¬ì¸íŠ¸ì—ì„œ 6ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
    features = []
    for point in radar_points:
        # ê·¹ì¢Œí‘œ ë³€í™˜
        range_val = np.sqrt(point.x**2 + point.y**2)
        azimuth = np.arctan2(point.y, point.x)
        
        # 6ì°¨ì› íŠ¹ì§• ë²¡í„°: [x, y, range, azimuth, velocity, snr]
        feature_vector = [
            point.x,
            point.y,
            range_val,
            azimuth,
            point.velocity,
            point.snr
        ]
        features.append(feature_vector)
    
    return np.array(features, dtype=np.float32)

def create_knn_edges(positions: np.ndarray, k: int = 6) -> np.ndarray:
    """k-NN ê¸°ë°˜ ê·¸ë˜í”„ ì—£ì§€ ìƒì„±"""
    n_points = len(positions)
    edges = []
    
    for i in range(n_points):
        # í˜„ì¬ í¬ì¸íŠ¸ì™€ ë‹¤ë¥¸ ëª¨ë“  í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
        distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))
        
        # ìê¸° ìì‹ ì„ ì œì™¸í•˜ê³  ê°€ì¥ ê°€ê¹Œìš´ kê°œ í¬ì¸íŠ¸ ì°¾ê¸°
        nearest_indices = np.argsort(distances)[1:k+1]  # ì²« ë²ˆì§¸ëŠ” ìê¸° ìì‹ ì´ë¯€ë¡œ ì œì™¸
        
        # ì–‘ë°©í–¥ ì—£ì§€ ì¶”ê°€
        for j in nearest_indices:
            edges.append([i, j])
            edges.append([j, i])
    
    # ì¤‘ë³µ ì œê±°
    edges = list(set(tuple(edge) for edge in edges))
    return np.array(edges).T if edges else np.array([[], []])

def load_radar_data(filename: str) -> Dict[float, RadarFrame]:
    """ë ˆì´ë” ë°ì´í„° ë¡œë“œ"""
    radar_frames = {}
    
    with open(filename, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                timestamp = float(parts[0])
                x = float(parts[1])
                y = float(parts[2])
                velocity = float(parts[3])
                snr = float(parts[4])
                
                point = RadarPoint(timestamp, x, y, velocity, snr)
                
                if timestamp not in radar_frames:
                    radar_frames[timestamp] = []
                radar_frames[timestamp].append(point)
    
    return radar_frames

def load_lidar_data(filename: str) -> Dict[float, LiDARFrame]:
    """LiDAR ë°ì´í„° ë¡œë“œ"""
    lidar_frames = {}
    
    with open(filename, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 4:
                timestamp = float(parts[0])
                x = float(parts[1])
                y = float(parts[2])
                intensity = float(parts[3])
                
                point = LiDARPoint(timestamp, x, y, intensity)
                
                if timestamp not in lidar_frames:
                    lidar_frames[timestamp] = []
                lidar_frames[timestamp].append(point)
    
    return lidar_frames

def create_labels_snr_distance(radar_frame: RadarFrame, lidar_frame: LiDARFrame, 
                              snr_threshold: float = 15.0, distance_threshold: float = 0.4) -> List[int]:
    """SNR + ê±°ë¦¬ ì¡°í•© ê¸°ë°˜ ë¼ë²¨ ìƒì„± (15dB + 0.4m)"""
    labels = []
    
    # LiDAR í¬ì¸íŠ¸ë“¤ì˜ ì¢Œí‘œ ë°°ì—´
    lidar_positions = np.array([[p.x, p.y] for p in lidar_frame])
    
    for radar_point in radar_frame:
        radar_pos = np.array([radar_point.x, radar_point.y])
        
        # ê°€ì¥ ê°€ê¹Œìš´ LiDAR í¬ì¸íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°
        if len(lidar_positions) > 0:
            distances = np.sqrt(np.sum((lidar_positions - radar_pos)**2, axis=1))
            min_distance = np.min(distances)
        else:
            min_distance = float('inf')
        
        # SNR + ê±°ë¦¬ ì¡°í•© ì¡°ê±´ (15dB + 0.4m)
        if min_distance <= distance_threshold and radar_point.snr >= snr_threshold:
            labels.append(1)  # ì‹¤ì œ íƒ€ê²Ÿ
        else:
            labels.append(0)  # ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ
    
    return labels

class GhostDetectorDataset:
    """ê³ ìŠ¤íŠ¸ íƒì§€ ë°ì´í„°ì…‹"""
    
    def __init__(self, radar_file: str, lidar_file: str, k: int = 6):
        self.k = k
        self.radar_frames = load_radar_data(radar_file)
        self.lidar_frames = load_lidar_data(lidar_file)
        self.graphs = []
        self.labels = []
        
        self._create_graphs()
    
    def _create_graphs(self):
        """ê·¸ë˜í”„ ë°ì´í„° ìƒì„±"""
        print("ğŸ“Š ê·¸ë˜í”„ ë°ì´í„° ìƒì„± ì¤‘...")
        
        total_real_targets = 0
        total_ghost_targets = 0
        
        for timestamp in self.radar_frames:
            if timestamp in self.lidar_frames:
                radar_frame = self.radar_frames[timestamp]
                lidar_frame = self.lidar_frames[timestamp]
                
                if len(radar_frame) < 2:  # ê·¸ë˜í”„ë¥¼ ë§Œë“¤ê¸°ì— í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ìŒ
                    continue
                
                # íŠ¹ì§• ì¶”ì¶œ
                features = extract_features(radar_frame)
                
                # ë¼ë²¨ ìƒì„± (SNR 15dB + ê±°ë¦¬ 0.4m)
                frame_labels = create_labels_snr_distance(radar_frame, lidar_frame, 
                                                        snr_threshold=15.0, distance_threshold=0.4)
                
                # k-NN ì—£ì§€ ìƒì„±
                positions = features[:, :2]  # x, y ì¢Œí‘œë§Œ ì‚¬ìš©
                edge_index = create_knn_edges(positions, k=self.k)
                
                if edge_index.size > 0:
                    # PyTorch Geometric ë°ì´í„° ìƒì„±
                    graph_data = Data(
                        x=torch.FloatTensor(features),
                        edge_index=torch.LongTensor(edge_index),
                        y=torch.FloatTensor(frame_labels)
                    )
                    
                    self.graphs.append(graph_data)
                    self.labels.extend(frame_labels)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    total_real_targets += sum(frame_labels)
                    total_ghost_targets += len(frame_labels) - sum(frame_labels)
        
        print(f"âœ… ì´ {len(self.graphs)}ê°œ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“Š ì‹¤ì œ íƒ€ê²Ÿ: {total_real_targets:,}ê°œ ({total_real_targets/(total_real_targets+total_ghost_targets)*100:.1f}%)")
        print(f"ğŸ‘» ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ: {total_ghost_targets:,}ê°œ ({total_ghost_targets/(total_real_targets+total_ghost_targets)*100:.1f}%)")
        print(f"ğŸ”— k-NN ì—°ê²°: k={self.k}")

def train_model():
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    print("ğŸš€ ë ˆì´ë” ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ íƒì§€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("âš™ï¸  ì„¤ì •: SNR â‰¥ 15dB + ê±°ë¦¬ â‰¤ 0.4m + k=6")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = GhostDetectorDataset('RadarMap_v2.txt', 'LiDARMap_v2.txt', k=6)
    
    if len(dataset.graphs) == 0:
        print("âŒ ê·¸ë˜í”„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    train_graphs, val_graphs = train_test_split(dataset.graphs, test_size=0.2, random_state=42)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_graphs, batch_size=16, shuffle=False)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = HybridGhostGNN(input_dim=6, hidden_dim=128, num_layers=3, dropout=0.1).to(device)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # í•™ìŠµ ê¸°ë¡
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    best_val_acc = 0.0
    patience_counter = 0
    max_patience = 15
    
    print(f"ğŸ¯ í•™ìŠµ ì‹œì‘ - ì´ {len(train_graphs)}ê°œ í•™ìŠµ ê·¸ë˜í”„, {len(val_graphs)}ê°œ ê²€ì¦ ê·¸ë˜í”„")
    
    for epoch in range(50):
        start_time = time.time()
        
        # í•™ìŠµ ë‹¨ê³„
        model.train()
        train_loss = 0.0
        train_predictions = []
        train_targets = []
        
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            
            outputs = model(batch.x, batch.edge_index, batch.batch)
            loss = criterion(outputs, batch.y)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            predictions = (outputs > 0.5).float()
            train_predictions.extend(predictions.cpu().numpy())
            train_targets.extend(batch.y.cpu().numpy())
        
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                outputs = model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(outputs, batch.y)
                
                val_loss += loss.item()
                
                predictions = (outputs > 0.5).float()
                val_predictions.extend(predictions.cpu().numpy())
                val_targets.extend(batch.y.cpu().numpy())
        
        # ì •í™•ë„ ê³„ì‚°
        train_acc = accuracy_score(train_targets, train_predictions)
        val_acc = accuracy_score(val_targets, val_predictions)
        
        # ê¸°ë¡ ì €ì¥
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss / len(val_loader))
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        scheduler.step(val_loss)
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save(model.state_dict(), 'ghost_detector_k6_15dB_0.4m.pth')
        else:
            patience_counter += 1
        
        epoch_time = time.time() - start_time
        
        print(f"Epoch {epoch+1:2d}/50 | "
              f"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_acc:.4f} | "
              f"Time: {epoch_time:.1f}s")
        
        if patience_counter >= max_patience:
            print(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ (patience={max_patience})")
            break
    
    print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")
    
    # í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
    create_training_visualization(train_losses, val_losses, train_accuracies, val_accuracies, dataset)

def create_training_visualization(train_losses, val_losses, train_accuracies, val_accuracies, dataset):
    """í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ì†ì‹¤ í•¨ìˆ˜ ê·¸ë˜í”„
    epochs = range(1, len(train_losses) + 1)
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    ax1.set_title('Training and Validation Loss (k=6, SNRâ‰¥15dB, distâ‰¤0.4m)', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì •í™•ë„ ê·¸ë˜í”„
    ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)
    ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)
    ax2.set_title('Training and Validation Accuracy', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ë°ì´í„° ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    all_labels = dataset.labels
    real_count = sum(all_labels)
    ghost_count = len(all_labels) - real_count
    
    categories = ['Real Targets', 'Ghost Targets']
    counts = [real_count, ghost_count]
    colors = ['green', 'red']
    
    bars = ax3.bar(categories, counts, color=colors, alpha=0.7)
    ax3.set_title('Target Distribution (k=6 neighbors)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Count')
    
    # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{count:,}\n({count/len(all_labels)*100:.1f}%)',
                ha='center', va='bottom', fontweight='bold')
    
    # 4. ëª¨ë¸ ì„¤ì • ì •ë³´
    ax4.axis('off')
    info_text = f"""
    ğŸ¯ ëª¨ë¸ ì„¤ì • (k=6 ê· í˜• ê¸°ì¤€)
    
    ğŸ“Š ë°ì´í„° í†µê³„:
    â€¢ ì´ ê·¸ë˜í”„: {len(dataset.graphs):,}ê°œ
    â€¢ ì´ ë…¸ë“œ: {len(all_labels):,}ê°œ
    â€¢ ì‹¤ì œ íƒ€ê²Ÿ: {real_count:,}ê°œ ({real_count/len(all_labels)*100:.1f}%)
    â€¢ ê³ ìŠ¤íŠ¸ íƒ€ê²Ÿ: {ghost_count:,}ê°œ ({ghost_count/len(all_labels)*100:.1f}%)
    
    âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:
    â€¢ SNR ì„ê³„ê°’: 15.0 dB (ë„ë©”ì¸ ì ì‘)
    â€¢ ê±°ë¦¬ ì„ê³„ê°’: 0.4 m (ì¤‘ê°„ ì§€ì )
    â€¢ k-NN ì—°ê²°: 6ê°œ (ê· í˜•ì¡íŒ ì´ì›ƒ)
    â€¢ ì€ë‹‰ì¸µ ì°¨ì›: 128
    â€¢ ë“œë¡­ì•„ì›ƒ: 0.1
    â€¢ í•™ìŠµë¥ : 0.005
    
    ğŸš€ ì„±ëŠ¥:
    â€¢ ìµœê³  ê²€ì¦ ì •í™•ë„: {max(val_accuracies):.4f}
    â€¢ ë””ë°”ì´ìŠ¤: {device}
    """
    
    ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig('k6_training_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š í•™ìŠµ ë¶„ì„ ê²°ê³¼ê°€ 'k6_training_analysis.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    train_model()
